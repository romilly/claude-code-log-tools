{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Claude Code Logs to PostgreSQL\n",
    "\n",
    "This notebook imports Claude Code conversation logs from JSONL files into a PostgreSQL database with full-text search support.\n",
    "\n",
    "## Schema Overview\n",
    "\n",
    "- **sessions** - One row per Claude Code session\n",
    "- **messages** - One row per log entry (user message, assistant response, system, etc.)\n",
    "- **content_blocks** - One row per content block within a message (text, tool_use, tool_result, thinking)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. PostgreSQL 13+ running (can be on a remote host)\n",
    "2. Create a `.env` file in the project root (copy from `.env.example`)\n",
    "3. Install dependencies: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T17:18:16.856277452Z",
     "start_time": "2026-01-11T17:18:16.393451062Z"
    }
   },
   "source": "import json\nimport os\nfrom pathlib import Path\nfrom uuid import UUID\n\nimport psycopg2\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Database configuration from environment\nDB_CONFIG = {\n    'host': os.getenv('CLAUDE_LOGS_DB_HOST', 'localhost'),\n    'port': int(os.getenv('CLAUDE_LOGS_DB_PORT', '5432')),\n    'database': os.getenv('CLAUDE_LOGS_DB_NAME', 'claude_logs'),\n    'user': os.getenv('CLAUDE_LOGS_DB_USER', 'postgres'),\n    'password': os.getenv('CLAUDE_LOGS_DB_PASSWORD', ''),\n}\n\nCLAUDE_LOGS_DIR = Path.home() / '.claude' / 'projects'\n\nprint(f\"Database host: {DB_CONFIG['host']}:{DB_CONFIG['port']}\")\nprint(f\"Database name: {DB_CONFIG['database']}\")\nprint(f\"Claude logs directory: {CLAUDE_LOGS_DIR}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database host: s2ag:5432\n",
      "Database name: claude_logs\n",
      "Claude logs directory: /home/romilly/.claude/projects\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T17:18:26.397127825Z",
     "start_time": "2026-01-11T17:18:26.002860021Z"
    }
   },
   "source": [
    "def test_connection():\n",
    "    \"\"\"Test database connection.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute('SELECT version()')\n",
    "        version = cur.fetchone()[0]\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(f\"Connected successfully!\")\n",
    "        print(f\"PostgreSQL version: {version}\")\n",
    "        return True\n",
    "    except psycopg2.OperationalError as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_connection()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!\n",
      "PostgreSQL version: PostgreSQL 13.15 (Raspbian 13.15-0+deb11u1) on arm-unknown-linux-gnueabihf, compiled by gcc (Raspbian 10.2.1-6+rpi1) 10.2.1 20210110, 32-bit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Schema\n",
    "\n",
    "Run this once to create the tables and indexes. Safe to re-run (uses IF NOT EXISTS)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:45:35.178404956Z",
     "start_time": "2025-12-22T11:45:34.994750106Z"
    }
   },
   "source": "SCHEMA_SQL = \"\"\"\n-- Sessions table: one row per Claude Code session\nCREATE TABLE IF NOT EXISTS sessions (\n    id SERIAL PRIMARY KEY,\n    session_uuid UUID UNIQUE NOT NULL,\n    project_path TEXT,\n    summary TEXT,\n    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n    total_input_tokens INT DEFAULT 0,\n    total_output_tokens INT DEFAULT 0\n);\n\n-- Messages table: one row per log entry (envelope only, content in content_blocks)\nCREATE TABLE IF NOT EXISTS messages (\n    id SERIAL PRIMARY KEY,\n    session_id INT NOT NULL REFERENCES sessions(id) ON DELETE CASCADE,\n    uuid TEXT,\n    type TEXT NOT NULL,           -- 'user', 'assistant', 'system', 'summary', etc.\n    role TEXT,                     -- 'user', 'assistant' (from message.role)\n    timestamp TIMESTAMPTZ,\n    cwd TEXT,\n    input_tokens INT,\n    output_tokens INT,\n    version TEXT\n);\n\n-- Content blocks table: one row per content block within a message\nCREATE TABLE IF NOT EXISTS content_blocks (\n    id SERIAL PRIMARY KEY,\n    message_id INT NOT NULL REFERENCES messages(id) ON DELETE CASCADE,\n    block_index INT NOT NULL,      -- order within the message\n    block_type TEXT NOT NULL,      -- 'text', 'tool_use', 'tool_result', 'thinking'\n    \n    -- Text content (for text, thinking, tool_result blocks)\n    text_content TEXT,\n    \n    -- Tool use fields\n    tool_name TEXT,                -- for tool_use blocks\n    tool_input JSONB,              -- for tool_use blocks (the input parameters)\n    tool_use_id TEXT,              -- links tool_use to its tool_result\n    \n    -- Full-text search on text content\n    content_tsvector tsvector GENERATED ALWAYS AS (\n        to_tsvector('english', COALESCE(text_content, ''))\n    ) STORED\n);\n\n-- Import metadata: tracks last import timestamp per project for idempotent imports\nCREATE TABLE IF NOT EXISTS import_metadata (\n    project_path TEXT PRIMARY KEY,\n    last_import_timestamp TIMESTAMPTZ NOT NULL\n);\n\n-- Indexes for messages\nCREATE INDEX IF NOT EXISTS idx_messages_session_id ON messages(session_id);\nCREATE INDEX IF NOT EXISTS idx_messages_type ON messages(type);\nCREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp DESC);\nCREATE INDEX IF NOT EXISTS idx_messages_session_timestamp ON messages(session_id, timestamp DESC);\n\n-- Unique index on messages.uuid as safety net for idempotent imports\nCREATE UNIQUE INDEX IF NOT EXISTS idx_messages_uuid_unique\n    ON messages(uuid) WHERE uuid IS NOT NULL;\n\n-- Indexes for content_blocks\nCREATE INDEX IF NOT EXISTS idx_content_blocks_message_id ON content_blocks(message_id);\nCREATE INDEX IF NOT EXISTS idx_content_blocks_type ON content_blocks(block_type);\nCREATE INDEX IF NOT EXISTS idx_content_blocks_tool_use_id ON content_blocks(tool_use_id);\nCREATE INDEX IF NOT EXISTS idx_content_blocks_tool_name ON content_blocks(tool_name);\nCREATE INDEX IF NOT EXISTS idx_content_blocks_fts ON content_blocks USING GIN(content_tsvector);\n\"\"\"\n\n# Migration SQL for existing databases (safe to re-run)\nMIGRATION_SQL = \"\"\"\n-- Add summary column if it doesn't exist\nALTER TABLE sessions ADD COLUMN IF NOT EXISTS summary TEXT;\n\n-- Create import_metadata table if it doesn't exist\nCREATE TABLE IF NOT EXISTS import_metadata (\n    project_path TEXT PRIMARY KEY,\n    last_import_timestamp TIMESTAMPTZ NOT NULL\n);\n\n-- Add unique index on messages.uuid\nCREATE UNIQUE INDEX IF NOT EXISTS idx_messages_uuid_unique\n    ON messages(uuid) WHERE uuid IS NOT NULL;\n\"\"\"\n\ndef create_schema():\n    \"\"\"Create database schema (and run migrations for existing databases).\"\"\"\n    conn = psycopg2.connect(**DB_CONFIG)\n    cur = conn.cursor()\n    cur.execute(SCHEMA_SQL)\n    cur.execute(MIGRATION_SQL)\n    conn.commit()\n    cur.close()\n    conn.close()\n    print(\"Schema created successfully!\")\n\ncreate_schema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Log Files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:45:45.285433882Z",
     "start_time": "2025-12-22T11:45:44.950251318Z"
    }
   },
   "source": [
    "def find_jsonl_files():\n",
    "    \"\"\"Discover all JSONL files in Claude Code log directory.\"\"\"\n",
    "    if not CLAUDE_LOGS_DIR.exists():\n",
    "        print(f\"Error: Claude logs directory not found at {CLAUDE_LOGS_DIR}\")\n",
    "        return []\n",
    "    \n",
    "    jsonl_files = list(CLAUDE_LOGS_DIR.glob('**/*.jsonl'))\n",
    "    print(f\"Found {len(jsonl_files)} JSONL files\")\n",
    "    return jsonl_files\n",
    "\n",
    "log_files = find_jsonl_files()\n",
    "\n",
    "# Show first few files\n",
    "for f in log_files[:10]:\n",
    "    print(f\"  {f.relative_to(CLAUDE_LOGS_DIR)}\")\n",
    "if len(log_files) > 10:\n",
    "    print(f\"  ... and {len(log_files) - 10} more\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220 JSONL files\n",
      "  -home-romilly-git-active-video-transcriber/agent-81b3a20c.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-a286637.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-aeac6d77.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-867d63b3.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-aed946a.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/f7f84c65-45e6-4255-949a-645d3f32f3a8.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/e91a98a1-1fcb-4f9f-904c-afd06ec20677.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-e021f857.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-a4b03da.jsonl\n",
      "  -home-romilly-git-active-video-transcriber/agent-92ee2965.jsonl\n",
      "  ... and 210 more\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:45:53.864282040Z",
     "start_time": "2025-12-22T11:45:53.840738223Z"
    }
   },
   "source": "def get_or_create_session(cur, session_uuid, project_path):\n    \"\"\"\n    Get session ID, creating it if needed.\n    Returns session_id.\n    \"\"\"\n    try:\n        session_uuid = UUID(session_uuid) if isinstance(session_uuid, str) else session_uuid\n    except (ValueError, AttributeError):\n        pass\n    \n    cur.execute(\n        \"\"\"\n        INSERT INTO sessions (session_uuid, project_path, created_at, updated_at)\n        VALUES (%s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n        ON CONFLICT (session_uuid) DO UPDATE SET updated_at = CURRENT_TIMESTAMP\n        RETURNING id\n        \"\"\",\n        (str(session_uuid), project_path)\n    )\n    return cur.fetchone()[0]\n\n\ndef get_last_import_timestamp(cur, project_path):\n    \"\"\"Get the last import timestamp for a project, or None if never imported.\"\"\"\n    cur.execute(\n        \"SELECT last_import_timestamp FROM import_metadata WHERE project_path = %s\",\n        (project_path,)\n    )\n    row = cur.fetchone()\n    return row[0] if row else None\n\n\ndef update_last_import_timestamp(cur, project_path, timestamp):\n    \"\"\"Update (upsert) the last import timestamp for a project.\"\"\"\n    cur.execute(\n        \"\"\"INSERT INTO import_metadata (project_path, last_import_timestamp)\n           VALUES (%s, %s)\n           ON CONFLICT (project_path) DO UPDATE SET last_import_timestamp = %s\"\"\",\n        (project_path, timestamp, timestamp)\n    )\n\n\ndef insert_content_blocks(cur, message_id, content):\n    \"\"\"\n    Insert content blocks for a message.\n    \n    Content can be:\n    - A string (user messages)\n    - A list of block dicts (assistant messages)\n    \n    Returns number of blocks inserted.\n    \"\"\"\n    if content is None:\n        return 0\n    \n    # Handle string content (user messages)\n    if isinstance(content, str):\n        if not content.strip():\n            return 0\n        cur.execute(\n            \"\"\"\n            INSERT INTO content_blocks (message_id, block_index, block_type, text_content)\n            VALUES (%s, %s, %s, %s)\n            \"\"\",\n            (message_id, 0, 'text', content)\n        )\n        return 1\n    \n    # Handle list of content blocks (assistant messages)\n    if not isinstance(content, list):\n        return 0\n    \n    block_count = 0\n    for idx, block in enumerate(content):\n        if not isinstance(block, dict):\n            continue\n        \n        block_type = block.get('type', 'unknown')\n        \n        if block_type == 'text':\n            cur.execute(\n                \"\"\"\n                INSERT INTO content_blocks (message_id, block_index, block_type, text_content)\n                VALUES (%s, %s, %s, %s)\n                \"\"\",\n                (message_id, idx, 'text', block.get('text', ''))\n            )\n            block_count += 1\n            \n        elif block_type == 'thinking':\n            cur.execute(\n                \"\"\"\n                INSERT INTO content_blocks (message_id, block_index, block_type, text_content)\n                VALUES (%s, %s, %s, %s)\n                \"\"\",\n                (message_id, idx, 'thinking', block.get('thinking', ''))\n            )\n            block_count += 1\n            \n        elif block_type == 'tool_use':\n            cur.execute(\n                \"\"\"\n                INSERT INTO content_blocks (\n                    message_id, block_index, block_type, \n                    tool_name, tool_input, tool_use_id\n                )\n                VALUES (%s, %s, %s, %s, %s, %s)\n                \"\"\",\n                (\n                    message_id, idx, 'tool_use',\n                    block.get('name'),\n                    json.dumps(block.get('input', {})),\n                    block.get('id')\n                )\n            )\n            block_count += 1\n            \n        elif block_type == 'tool_result':\n            # tool_result content can be string or list\n            result_content = block.get('content', '')\n            if isinstance(result_content, list):\n                # Extract text from content blocks within the result\n                result_content = '\\n'.join(\n                    item.get('text', '') for item in result_content \n                    if isinstance(item, dict) and item.get('type') == 'text'\n                )\n            \n            cur.execute(\n                \"\"\"\n                INSERT INTO content_blocks (\n                    message_id, block_index, block_type, \n                    text_content, tool_use_id\n                )\n                VALUES (%s, %s, %s, %s, %s)\n                \"\"\",\n                (\n                    message_id, idx, 'tool_result',\n                    result_content,\n                    block.get('tool_use_id')\n                )\n            )\n            block_count += 1\n    \n    return block_count\n\n\ndef import_jsonl_file(filepath, conn):\n    \"\"\"\n    Import a single JSONL file into the database.\n    Idempotent: skips entries already imported based on timestamp tracking.\n    Returns: (message_count, block_count, error_count, skipped_count)\n    \"\"\"\n    from datetime import datetime, timezone\n    \n    cur = conn.cursor()\n    message_count = 0\n    block_count = 0\n    error_count = 0\n    skipped_count = 0\n    session_id = None\n    summary_text = None\n    max_timestamp = None\n    \n    try:\n        project_path = str(filepath.parent.relative_to(CLAUDE_LOGS_DIR))\n    except ValueError:\n        project_path = str(filepath.parent)\n    \n    # Get the cutoff timestamp for this project\n    cutoff = get_last_import_timestamp(cur, project_path)\n    \n    try:\n        with open(filepath, 'r') as f:\n            for line_num, line in enumerate(f, 1):\n                try:\n                    data = json.loads(line)\n                    entry_type = data.get('type')\n                    \n                    # Skip file-history-snapshot entries entirely\n                    if entry_type == 'file-history-snapshot':\n                        skipped_count += 1\n                        continue\n                    \n                    # Capture summary text (no timestamp, always process)\n                    if entry_type == 'summary':\n                        summary_text = data.get('summary')\n                        skipped_count += 1  # not a message we insert\n                        continue\n                    \n                    # Timestamp-based filtering for regular entries\n                    entry_timestamp = data.get('timestamp')\n                    if entry_timestamp and cutoff:\n                        # Parse ISO timestamp for comparison\n                        entry_dt = datetime.fromisoformat(entry_timestamp.replace('Z', '+00:00'))\n                        if entry_dt <= cutoff:\n                            skipped_count += 1\n                            continue\n                    \n                    # Track max timestamp for updating cutoff after import\n                    if entry_timestamp:\n                        entry_dt = datetime.fromisoformat(entry_timestamp.replace('Z', '+00:00'))\n                        if max_timestamp is None or entry_dt > max_timestamp:\n                            max_timestamp = entry_dt\n                    \n                    # Create or get session\n                    if data.get('sessionId') and session_id is None:\n                        session_id = get_or_create_session(\n                            cur, \n                            data['sessionId'],\n                            project_path\n                        )\n                    \n                    if not session_id:\n                        continue\n                    \n                    # Extract message metadata\n                    message = data.get('message', {})\n                    usage = message.get('usage', {}) if isinstance(message, dict) else {}\n                    \n                    # Insert message with ON CONFLICT safety net\n                    msg_uuid = data.get('uuid')\n                    if msg_uuid:\n                        cur.execute(\n                            \"\"\"\n                            INSERT INTO messages (\n                                session_id, uuid, type, role,\n                                timestamp, cwd, input_tokens, output_tokens, version\n                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n                            ON CONFLICT (uuid) WHERE uuid IS NOT NULL DO NOTHING\n                            RETURNING id\n                            \"\"\",\n                            (\n                                session_id,\n                                msg_uuid,\n                                entry_type,\n                                message.get('role') if isinstance(message, dict) else None,\n                                entry_timestamp,\n                                data.get('cwd'),\n                                usage.get('input_tokens') if isinstance(usage, dict) else None,\n                                usage.get('output_tokens') if isinstance(usage, dict) else None,\n                                data.get('version')\n                            )\n                        )\n                        result = cur.fetchone()\n                        if result is None:\n                            # Already existed (conflict), skip\n                            skipped_count += 1\n                            continue\n                        message_id = result[0]\n                    else:\n                        cur.execute(\n                            \"\"\"\n                            INSERT INTO messages (\n                                session_id, uuid, type, role,\n                                timestamp, cwd, input_tokens, output_tokens, version\n                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n                            RETURNING id\n                            \"\"\",\n                            (\n                                session_id,\n                                None,\n                                entry_type,\n                                message.get('role') if isinstance(message, dict) else None,\n                                entry_timestamp,\n                                data.get('cwd'),\n                                usage.get('input_tokens') if isinstance(usage, dict) else None,\n                                usage.get('output_tokens') if isinstance(usage, dict) else None,\n                                data.get('version')\n                            )\n                        )\n                        message_id = cur.fetchone()[0]\n                    \n                    message_count += 1\n                    \n                    # Insert content blocks\n                    content = message.get('content') if isinstance(message, dict) else None\n                    block_count += insert_content_blocks(cur, message_id, content)\n                    \n                    # Commit every 100 messages\n                    if message_count % 100 == 0:\n                        conn.commit()\n                \n                except (json.JSONDecodeError, ValueError) as e:\n                    error_count += 1\n                    if error_count <= 3:\n                        print(f\"  Error at line {line_num}: {e}\")\n                    continue\n        \n        # Final commit\n        conn.commit()\n        \n        # Update session summary if we found one\n        if session_id and summary_text:\n            cur.execute(\n                \"UPDATE sessions SET summary = %s WHERE id = %s\",\n                (summary_text, session_id)\n            )\n            conn.commit()\n        \n        # Update session token totals\n        if session_id:\n            cur.execute(\n                \"\"\"\n                UPDATE sessions \n                SET total_input_tokens = (\n                    SELECT COALESCE(SUM(input_tokens), 0) FROM messages \n                    WHERE session_id = %s\n                ),\n                total_output_tokens = (\n                    SELECT COALESCE(SUM(output_tokens), 0) FROM messages \n                    WHERE session_id = %s\n                )\n                WHERE id = %s\n                \"\"\",\n                (session_id, session_id, session_id)\n            )\n            conn.commit()\n        \n        # Update last import timestamp for this project\n        if max_timestamp:\n            update_last_import_timestamp(cur, project_path, max_timestamp)\n            conn.commit()\n        \n        return message_count, block_count, error_count, skipped_count\n    \n    except Exception as e:\n        print(f\"  Failed to import: {e}\")\n        conn.rollback()\n        return 0, 0, -1, 0\n    \n    finally:\n        cur.close()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:47:56.100716782Z",
     "start_time": "2025-12-22T11:46:13.067448148Z"
    }
   },
   "source": "def import_all_logs(files):\n    \"\"\"Import all discovered log files. Idempotent - safe to re-run.\"\"\"\n    conn = psycopg2.connect(**DB_CONFIG)\n    \n    total_messages = 0\n    total_blocks = 0\n    total_errors = 0\n    total_skipped = 0\n    \n    for i, filepath in enumerate(files, 1):\n        messages, blocks, errors, skipped = import_jsonl_file(filepath, conn)\n        total_messages += messages\n        total_blocks += blocks\n        total_skipped += skipped\n        if errors > 0:\n            total_errors += errors\n        if messages > 0 or errors > 0:\n            print(f\"[{i}/{len(files)}] {filepath.name}\")\n            print(f\"  -> {messages} messages, {blocks} blocks, {skipped} skipped ({errors} errors)\")\n    \n    conn.close()\n    \n    print(f\"\\n=== Import Complete ===\")\n    print(f\"New messages: {total_messages}\")\n    print(f\"New content blocks: {total_blocks}\")\n    print(f\"Skipped entries: {total_skipped}\")\n    print(f\"Errors: {total_errors}\")\n\n# Run the import\nimport_all_logs(log_files)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T17:18:59.156754708Z",
     "start_time": "2026-01-11T17:18:58.999017852Z"
    }
   },
   "source": "def show_stats():\n    \"\"\"Show database statistics.\"\"\"\n    conn = psycopg2.connect(**DB_CONFIG)\n    cur = conn.cursor()\n    \n    cur.execute(\"SELECT COUNT(*) FROM sessions\")\n    session_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM sessions WHERE summary IS NOT NULL\")\n    sessions_with_summary = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM messages\")\n    message_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM content_blocks\")\n    block_count = cur.fetchone()[0]\n    \n    cur.execute(\"\"\"\n        SELECT block_type, COUNT(*) \n        FROM content_blocks \n        GROUP BY block_type \n        ORDER BY COUNT(*) DESC\n    \"\"\")\n    block_types = cur.fetchall()\n    \n    cur.execute(\"\"\"\n        SELECT \n            COALESCE(SUM(input_tokens), 0) as total_input,\n            COALESCE(SUM(output_tokens), 0) as total_output\n        FROM messages\n    \"\"\")\n    tokens = cur.fetchone()\n    \n    cur.execute(\"SELECT COUNT(*) FROM import_metadata\")\n    tracked_projects = cur.fetchone()[0]\n    \n    print(f\"Sessions: {session_count} ({sessions_with_summary} with summaries)\")\n    print(f\"Messages: {message_count}\")\n    print(f\"Content blocks: {block_count}\")\n    print(f\"\\nBlock types:\")\n    for block_type, count in block_types:\n        print(f\"  {block_type}: {count}\")\n    print(f\"\\nTotal tokens: {tokens[0] + tokens[1]:,}\")\n    print(f\"  Input: {tokens[0]:,}\")\n    print(f\"  Output: {tokens[1]:,}\")\n    print(f\"\\nTracked projects: {tracked_projects}\")\n    \n    cur.close()\n    conn.close()\n\nshow_stats()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test: Full-Text Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T17:22:11.803476411Z",
     "start_time": "2026-01-11T17:22:11.593921876Z"
    }
   },
   "source": [
    "def search(query_text, limit=5):\n",
    "    \"\"\"Search text content across all block types.\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            cb.id,\n",
    "            cb.block_type,\n",
    "            m.timestamp,\n",
    "            m.type as message_type,\n",
    "            LEFT(cb.text_content, 200) as content_preview,\n",
    "            ts_rank(cb.content_tsvector, q.query) AS relevance\n",
    "        FROM content_blocks cb\n",
    "        JOIN messages m ON cb.message_id = m.id,\n",
    "             plainto_tsquery('english', %s) q(query)\n",
    "        WHERE cb.content_tsvector @@ q.query\n",
    "        ORDER BY relevance DESC, m.timestamp DESC\n",
    "        LIMIT %s\n",
    "    \"\"\", (query_text, limit))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(f\"Search: '{query_text}' ({len(results)} results)\\n\")\n",
    "    for block_id, block_type, timestamp, msg_type, preview, relevance in results:\n",
    "        print(f\"[{relevance:.3f}] {timestamp} ({msg_type}/{block_type})\")\n",
    "        print(f\"  {preview}...\")\n",
    "        print()\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Try a search\n",
    "search('Hexagonal', 20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search: 'Hexagonal' (20 results)\n",
      "\n",
      "[0.098] 2025-12-17 15:56:54.592000+00:00 (user/tool_result)\n",
      "  Web search results for query: \"Hexagonal Architecture Ports and Adapters Alistair Cockburn history origins 2005\"\n",
      "\n",
      "Links: [{\"title\":\"Hexagonal architecture (software) - Wikipedia\",\"url\":\"https://en.wik...\n",
      "\n",
      "[0.097] 2025-12-08 09:59:38.032000+00:00 (user/tool_result)\n",
      "  Excellent! Now I have all the information I need. Let me compile a comprehensive recommendations report:\n",
      "\n",
      "## Hexagonal Architecture Implementation Recommendations for Transcriber Application\n",
      "\n",
      "Based on...\n",
      "\n",
      "[0.097] 2025-12-08 09:59:37.917000+00:00 (assistant/text)\n",
      "  Excellent! Now I have all the information I need. Let me compile a comprehensive recommendations report:\n",
      "\n",
      "## Hexagonal Architecture Implementation Recommendations for Transcriber Application\n",
      "\n",
      "Based on...\n",
      "\n",
      "[0.097] 2025-12-08 09:57:17.199000+00:00 (user/tool_result)\n",
      "  Web search results for query: \"hexagonal architecture Python best practices 2025\"\n",
      "\n",
      "Links: [{\"title\":\"Structure a Python project in hexagonal architecture using AWS Lambda - AWS Prescriptive Guidance\",...\n",
      "\n",
      "[0.096] 2025-12-08 09:57:18.796000+00:00 (user/tool_result)\n",
      "  Web search results for query: \"refactoring to hexagonal architecture incrementally Python TDD\"\n",
      "\n",
      "Links: [{\"title\":\"Refactoring to Hexagonal Architecture Course by Ted M. Young\",\"url\":\"https://ted.dev/r...\n",
      "\n",
      "[0.096] 2025-12-08 09:57:40.688000+00:00 (user/tool_result)\n",
      "  Web search results for query: \"hexagonal architecture Python directory structure domain application infrastructure\"\n",
      "\n",
      "Links: [{\"title\":\"Structure a Python project in hexagonal architecture using AWS La...\n",
      "\n",
      "[0.094] 2025-12-08 14:13:00.480000+00:00 (user/tool_result)\n",
      "       1→# Hexagonal Architecture Refactoring Plan with TDD\n",
      "     2→\n",
      "     3→## Overview\n",
      "     4→\n",
      "     5→Refactor the transcriber application to hexagonal architecture (ports & adapters) to improve testabi...\n",
      "\n",
      "[0.094] 2025-12-08 10:47:23.606000+00:00 (user/tool_result)\n",
      "       1→# Hexagonal Architecture Refactoring Plan with TDD\n",
      "     2→\n",
      "     3→## Overview\n",
      "     4→\n",
      "     5→Refactor the transcriber application to hexagonal architecture (ports & adapters) to improve testabi...\n",
      "\n",
      "[0.094] 2025-12-08 10:26:04.887000+00:00 (user/tool_result)\n",
      "  User has approved your plan. You can now start coding. Start with updating your todo list if applicable\n",
      "\n",
      "Your plan has been saved to: /home/romilly/.claude/plans/stateless-launching-storm.md\n",
      "You can r...\n",
      "\n",
      "[0.094] 2025-11-23 10:01:39.630000+00:00 (user/tool_result)\n",
      "  The file /home/romilly/git/active/project-database/CLAUDE.md has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n",
      "     1→# CLAUDE.md\n",
      "     2→\n",
      "     3→This file provid...\n",
      "\n",
      "[0.094] 2025-11-23 09:57:01.816000+00:00 (user/tool_result)\n",
      "       1→# CLAUDE.md\n",
      "     2→\n",
      "     3→This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n",
      "     4→\n",
      "     5→## Project Overview\n",
      "     6→\n",
      "     7→**project-dat...\n",
      "\n",
      "[0.093] 2025-12-01 16:44:36.309000+00:00 (user/text)\n",
      "  This session is being continued from a previous conversation that ran out of context. The conversation is summarized below:\n",
      "Analysis:\n",
      "Let me chronologically analyze this conversation, which is a conti...\n",
      "\n",
      "[0.093] 2025-11-29 13:55:46.388000+00:00 (user/tool_result)\n",
      "       1→# Project Database - Progress Report\n",
      "     2→**Date:** November 19, 2025\n",
      "     3→\n",
      "     4→## Summary\n",
      "     5→Built a production-ready README generation system using hexagonal architecture (ports an...\n",
      "\n",
      "[0.093] 2025-11-23 09:56:51.003000+00:00 (user/tool_result)\n",
      "       1→# Project Database - Progress Report\n",
      "     2→**Date:** November 19, 2025\n",
      "     3→\n",
      "     4→## Summary\n",
      "     5→Built a production-ready README generation system using hexagonal architecture (ports an...\n",
      "\n",
      "[0.092] 2025-12-08 09:58:32.295000+00:00 (user/tool_result)\n",
      "  Based on my thorough exploration of the testing infrastructure in the transcriber project, here are my findings:\n",
      "\n",
      "## Testing Infrastructure Assessment\n",
      "\n",
      "### What Exists Currently\n",
      "\n",
      "**1. Test Directory S...\n",
      "\n",
      "[0.092] 2025-12-08 09:58:32.234000+00:00 (assistant/text)\n",
      "  Based on my thorough exploration of the testing infrastructure in the transcriber project, here are my findings:\n",
      "\n",
      "## Testing Infrastructure Assessment\n",
      "\n",
      "### What Exists Currently\n",
      "\n",
      "**1. Test Directory S...\n",
      "\n",
      "[0.092] 2025-12-08 09:57:18.214000+00:00 (user/tool_result)\n",
      "  Web search results for query: \"Python ports and adapters pattern implementation ABC Protocol\"\n",
      "\n",
      "Links: [{\"title\":\"Python Nameko ports and adapters (hexagonal) architecture example\",\"url\":\"https://jorze...\n",
      "\n",
      "[0.091] 2025-12-02 15:12:47.464000+00:00 (user/text)\n",
      "  This session is being continued from a previous conversation that ran out of context. The conversation is summarized below:\n",
      "Analysis:\n",
      "Let me chronologically analyze this conversation, which is a conti...\n",
      "\n",
      "[0.089] 2025-12-08 10:07:24.809000+00:00 (user/tool_result)\n",
      "  I understand - I'm in read-only mode and cannot write files. Let me provide you with a comprehensive implementation plan directly in my response.\n",
      "\n",
      "---\n",
      "\n",
      "# Hexagonal Architecture Refactoring Plan with T...\n",
      "\n",
      "[0.089] 2025-12-08 10:07:24.706000+00:00 (assistant/text)\n",
      "  I understand - I'm in read-only mode and cannot write files. Let me provide you with a comprehensive implementation plan directly in my response.\n",
      "\n",
      "---\n",
      "\n",
      "# Hexagonal Architecture Refactoring Plan with T...\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Failed Tool Calls\n",
    "\n",
    "Example query to find tool calls that resulted in errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:49:23.961754780Z",
     "start_time": "2025-12-22T11:49:23.839689180Z"
    }
   },
   "source": [
    "def find_failed_tool_calls(limit=10):\n",
    "    \"\"\"Find tool calls where the result contains error indicators.\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            m.timestamp,\n",
    "            s.project_path,\n",
    "            cb_call.tool_name,\n",
    "            LEFT(cb_result.text_content, 300) as result_preview\n",
    "        FROM content_blocks cb_result\n",
    "        JOIN content_blocks cb_call \n",
    "            ON cb_call.tool_use_id = cb_result.tool_use_id \n",
    "            AND cb_call.block_type = 'tool_use'\n",
    "        JOIN messages m ON cb_result.message_id = m.id\n",
    "        JOIN sessions s ON m.session_id = s.id\n",
    "        WHERE cb_result.block_type = 'tool_result'\n",
    "          AND cb_result.content_tsvector @@ to_tsquery('english', \n",
    "              'error | failed | exception | traceback | denied'\n",
    "          )\n",
    "        ORDER BY m.timestamp DESC\n",
    "        LIMIT %s\n",
    "    \"\"\", (limit,))\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(f\"Found {len(results)} failed tool calls:\\n\")\n",
    "    for timestamp, project, tool_name, preview in results:\n",
    "        print(f\"{timestamp} [{project}]\")\n",
    "        print(f\"  Tool: {tool_name}\")\n",
    "        print(f\"  Result: {preview}...\")\n",
    "        print()\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "find_failed_tool_calls()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 failed tool calls:\n",
      "\n",
      "2025-12-22 11:34:12.594000+00:00 [-home-romilly-git-active-claude-code-log-tools]\n",
      "  Tool: Write\n",
      "  Result: The file /home/romilly/git/active/claude-code-log-tools/notebooks/01_import_logs.ipynb has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n",
      "     1→{\n",
      "     2→ \"cells\": [\n",
      "     3→  {\n",
      "     4→   \"cell_type\": \"markdown\",\n",
      "     5→   \"metadata\": {},\n",
      "     6→   \"source\": [\n",
      "  ...\n",
      "\n",
      "2025-12-22 11:32:32.674000+00:00 [-home-romilly-git-active-claude-code-log-tools]\n",
      "  Tool: Read\n",
      "  Result: <cell id=\"cell-0\"><cell_type>markdown</cell_type># Import Claude Code Logs to PostgreSQL\n",
      "\n",
      "This notebook imports Claude Code conversation logs from JSONL files into a PostgreSQL database with full-text search support.\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "1. PostgreSQL 13+ running (can be on a remote host)\n",
      "2. Create a ...\n",
      "\n",
      "2025-12-22 11:15:41.162000+00:00 [-home-romilly-git-active-claude-code-log-tools]\n",
      "  Tool: Read\n",
      "  Result:      1→# PostgreSQL Full-Text Search Setup for Claude Code Logs\n",
      "     2→\n",
      "     3→A practical guide to storing and searching Claude Code conversation logs using PostgreSQL with native full-text search (`tsvector`). This setup prioritizes speed and simplicity—no fuzzy matching, just fast exact-match ful...\n",
      "\n",
      "2025-12-21 15:06:39.369000+00:00 [-home-romilly-git-active-video-project-textual-gui]\n",
      "  Tool: Edit\n",
      "  Result: The file /home/romilly/git/active/video-project-textual-gui/CLAUDE.md has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n",
      "    18→When calling an API you haven't used before in this project:\n",
      "    19→1. Fetch the official documentation immediately before writing the...\n",
      "\n",
      "2025-12-21 11:33:10.005000+00:00 [-home-romilly-git-active-weekly-planning]\n",
      "  Tool: Bash\n",
      "  Result: dd8b3de Added dataflow diagram.\n",
      "f44ed5f Update progress report for 2025-12-18\n",
      "e775d5f Release v0.2.0 on PyPI\n",
      "6d90281 Update demo script and README to use test data\n",
      "dc9bd34 Fix gitignore path for tests/data/generated\n",
      "9933b6b Fix integration tests to use tracked test data\n",
      "9f94623 Preparing for package...\n",
      "\n",
      "2025-12-21 11:32:31.101000+00:00 [-home-romilly-git-active-weekly-planning]\n",
      "  Tool: Read\n",
      "  Result:      1→- Journal\n",
      "     2→- [[MoSCoW]]\n",
      "     3→\t- #must (**timebox**)\n",
      "     4→\t\t- DONE cancel Vocal Image\n",
      "     5→\t\t  :LOGBOOK:\n",
      "     6→\t\t  CLOCK: [2025-12-15 Mon 13:45:02]--[2025-12-15 Mon 13:45:02] =>  00:00:00\n",
      "     7→\t\t  :END:\n",
      "     8→\t\t- DONE Add Gui in [[project/video-transcriber-gui]]\n",
      "     9→\t\t  :LOG...\n",
      "\n",
      "2025-12-21 11:05:41.884000+00:00 [-home-romilly-git-active-video-transcriber]\n",
      "  Tool: Task\n",
      "  Result: Perfect! Now I have a comprehensive understanding of the codebase. Let me create a detailed summary report.\n",
      "\n",
      "## Video-Transcriber Complete Data Flow Analysis\n",
      "\n",
      "I've thoroughly explored the video-transcriber codebase. Here's a comprehensive breakdown of the complete architecture and data flow:\n",
      "\n",
      "### 1....\n",
      "\n",
      "2025-12-21 11:04:55.111000+00:00 [-home-romilly-git-active-video-transcriber]\n",
      "  Tool: Read\n",
      "  Result:      1→\"\"\"Integration tests for audio adapters.\n",
      "     2→\n",
      "     3→These tests require:\n",
      "     4→- ffmpeg installed on the system\n",
      "     5→- A test video file with audio\n",
      "     6→- faster-whisper installed\n",
      "     7→\"\"\"\n",
      "     8→\n",
      "     9→import pytest\n",
      "    10→import tempfile\n",
      "    11→from pathlib import Path\n",
      "    12→\n",
      " ...\n",
      "\n",
      "2025-12-21 11:04:55.110000+00:00 [-home-romilly-git-active-video-transcriber]\n",
      "  Tool: Read\n",
      "  Result:      1→\"\"\"Integration tests for OpenCVVideoAdapter.\n",
      "     2→\n",
      "     3→These tests require a test video file to be present.\n",
      "     4→\"\"\"\n",
      "     5→\n",
      "     6→import numpy as np\n",
      "     7→import pytest\n",
      "     8→from pathlib import Path\n",
      "     9→\n",
      "    10→from video_transcriber.adapters.opencv_video import OpenCVVideoAdap...\n",
      "\n",
      "2025-12-21 11:04:52.896000+00:00 [-home-romilly-git-active-video-transcriber]\n",
      "  Tool: Read\n",
      "  Result:      1→\"\"\"Fake implementations of audio ports for testing.\"\"\"\n",
      "     2→\n",
      "     3→from video_transcriber.ports.audio_extractor import AudioExtractionError\n",
      "     4→from video_transcriber.ports.audio_transcriber import AudioTranscriptionError\n",
      "     5→from video_transcriber.domain.models import AudioSegment\n",
      " ...\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Usage Summary\n",
    "\n",
    "See which tools are used most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_usage_summary():\n",
    "    \"\"\"Show tool usage statistics.\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "        SELECT tool_name, COUNT(*) as usage_count\n",
    "        FROM content_blocks\n",
    "        WHERE block_type = 'tool_use'\n",
    "          AND tool_name IS NOT NULL\n",
    "        GROUP BY tool_name\n",
    "        ORDER BY usage_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    print(\"Tool usage summary:\\n\")\n",
    "    for tool_name, count in results:\n",
    "        print(f\"  {tool_name}: {count}\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "tool_usage_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}